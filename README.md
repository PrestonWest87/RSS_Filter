# ðŸ“° RSS_Filter

A lightweight, human-in-the-loop news aggregator and smart filter. This tool ingests hundreds of RSS feeds concurrently, scores them based on custom keyword weights, and bubbles up the most important articles for quick identification. It features a built-in Machine Learning engine that learns from your reading habits to automatically filter the noise and highlight what matters.

## ðŸ—ï¸ Architecture
* **Frontend:** Streamlit (Python)
* **Backend Worker:** Python `schedule` with multithreaded `requests`
* **Database:** PostgreSQL 15
* **AI/ML:** Scikit-Learn (TF-IDF Vectorizer + Naive Bayes Classifier)
* **Deployment:** Docker Compose

## ðŸ“– Usage Guide

The application is built around a rapid triage workflow.

1.  **Configuration:** Use the bulk-add text areas to load your RSS feed URLs and define your initial Keywords and Weights.
2.  **Priority Queue:** The `Dashboard` will display unreviewed articles that scored above the visibility threshold. 
    * Click **âœ… Keep** for important items.
    * Click **âŒ Dismiss** for noise/irrelevant items.
3.  **The Archive:** Dismissed articles vanish from your queue. Kept articles are moved to the `Saved Articles` tab for reading or permanent record-keeping.
## âš™ï¸ System Requirements

This application is highly optimized and designed to run efficiently on minimal hardware. The requirements below are based on real-world telemetry with 100+ active RSS feeds and the Machine Learning engine active.

### **Minimum Hardware**
* **CPU:** 1 Core
* **RAM:** 1 GB (Application consumes ~550 MB under active load)
* **Storage:** 5 GB (Accommodates Docker images and PostgreSQL text storage)

### **Recommended Hardware (For smooth ML training & concurrent fetching)**
* **CPU:** 2+ Cores (UI operations like model retraining can briefly utilize >100% of a single thread)
* **RAM:** 2 GB 
* **Storage:** 10 GB SSD (Improves database read/write speeds for large archives)

### **Software Requirements**
* **Docker:** Engine v20.10.0 or higher
* **Docker Compose:** v2.0.0 or higher
* **OS:** Any Linux distribution (Ubuntu/Debian recommended), Windows (via WSL2), or macOS.

### **Resource Allocation Breakdown**
Typical baseline memory consumption per container:
* `web` (Streamlit + ML Brain): ~250 MB - 300 MB
* `worker` (Multithreaded Fetcher): ~200 MB - 250 MB
* `db` (PostgreSQL 15): ~40 MB - 60 MB

## ðŸš€ Installation & Deployment

1. **Clone the repository** and navigate to the project folder.
2. **Set up environment variables:** Edit the `.env` file to change default database passwords if deploying to production.
3. **Build and start the containers:**
```bash
docker-compose up --build -d

```


4. **Access the Dashboard:** Open a web browser and navigate to `http://localhost:8501`.

3. **The Archive:** Dismissed articles vanish from the UI. Acknowledged articles are moved to the `Acknowledged (Confirmed)` tab for permanent record-keeping.

## ðŸ§  Machine Learning & Tuning

This system suffers from the "Cold Start Problem"â€”the ML model knows nothing until you teach it.

### Phase 1: Rule-Based (The Default)

Out of the box, the system uses the `KeywordScorer`. It scans text for your configured keywords and adds up the weights. If `Total Score >= 45`, it bubbles the article to the Inbox.

### Phase 2: Training the Brain

Every time you click "Acknowledge" or "Dismiss", you are labeling training data.

1. Review at least 50-100 articles manually using the UI to build a solid dataset.
2. Navigate to the `Training Data` tab.
3. Click **ðŸš€ Retrain Model Now**.
4. The system will generate a Naive Bayes model (`ml_model.pkl`) based on your specific operational preferences.

### Phase 3: ML Takeover

Once `ml_model.pkl` exists, the backend worker automatically detects it and switches from `KeywordScorer` to `MLScorer`. The ML model evaluates the contextual probability of an article being important, rather than relying on rigid keyword matching.

* *Note: You can always fall back to rule-based scoring by deleting the `ml_model.pkl` file and restarting the worker.*

## ðŸ› ï¸ Troubleshooting & Commands

**View Live Worker Logs (To monitor RSS scraping):**

```bash
docker-compose logs -f worker

```

**Restart the Worker (Required after manual code changes):**

```bash
docker-compose restart worker

```

**System Reset:**
If the database becomes bloated or you want to start fresh, navigate to `Configuration` -> `Danger Zone` in the UI to perform a safe SQL truncation of the articles table, or a complete factory reset.

---

## ðŸ¤– Addendum: AI-Generated Codebase

Please note that the entirety of this application's codebase was generated by Artificial Intelligence.

The Python backend, Streamlit frontend, Scikit-Learn machine learning logic, PostgreSQL database schema, and Docker deployment configurations were written by an AI assistant (Google's Gemini) based on iterative prompting.

While the code was AI-generated, the system architecture, feature requirements, operational workflow (such as the "Inbox Zero" methodology), and rigorous debugging were orchestrated and directed entirely by a human engineer. This project serves as a practical demonstration of AI-assisted software development and rapid prototyping for critical infrastructure monitoring.
