import time
import schedule
import feedparser
import requests
import sys
import concurrent.futures
from sqlalchemy.exc import IntegrityError
from src.database import SessionLocal, Article, FeedSource, init_db
from src.config import ALERT_THRESHOLD
from src.logic import get_scorer
from datetime import datetime

# Initialize DB on startup
init_db()

def log(message, source="SYSTEM"):
    """Helper to force print logs instantly in Docker"""
    print(f"[{datetime.now().strftime('%H:%M:%S')}] [{source.upper()}] {message}")
    sys.stdout.flush()

def process_single_feed(source_id, source_name, source_url, scorer):
    """Worker function to process a single feed in its own thread."""
    # Every thread MUST have its own database session
    session = SessionLocal()
    total_added = 0
    
    try:
        # 1. Fetch with a strict timeout and a real browser User-Agent
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        # Timeout: 5 seconds to connect, 10 seconds to read the data
        response = requests.get(source_url, headers=headers, timeout=(5, 10))
        response.raise_for_status() # Throw error if we get a 404 or 403 Forbidden
        
        # 2. Parse the raw XML content
        feed = feedparser.parse(response.content)
        seen_links_in_batch = set()

        for entry in feed.entries:
            link = entry.get('link', '')
            if not link:
                continue
                
            # DB Deduplication
            if session.query(Article).filter_by(link=link).first():
                continue

            # Batch Deduplication
            if link in seen_links_in_batch:
                continue
            seen_links_in_batch.add(link)

            # Process Article
            title = entry.get('title', '')
            summary = entry.get('summary', '')
            full_text = f"{title} {summary}"
            
            score, reasons = scorer.score(full_text)
            
            article = Article(
                title=title,
                link=link,
                summary=summary,
                source=source_name,
                published_date=datetime.utcnow(),
                score=float(score),
                keywords_found=reasons,
                is_bubbled=(score >= ALERT_THRESHOLD)
            )
            
            session.add(article)
            total_added += 1
        
        session.commit()
        if total_added > 0:
            log(f"‚úÖ {source_name}: Found {total_added} new articles.", "WORKER")
            
    except requests.exceptions.Timeout:
        log(f"‚è≥ Timeout skipping {source_name}", "WORKER")
    except requests.exceptions.HTTPError as e:
        log(f"üîí Blocked/Error on {source_name}: {e.response.status_code}", "WORKER")
    except Exception as e:
        session.rollback()
        log(f"‚ùå Error on {source_name}: {e}", "WORKER")
    finally:
        # ALWAYS close the session to prevent database lockups
        session.close()
        
    return total_added

def fetch_feeds(source="Scheduled"):
    log("üöÄ Starting concurrent feed fetch cycle...", source)
    
    # Use a temporary session just to get the list of active feeds
    main_session = SessionLocal()
    sources = main_session.query(FeedSource).filter(FeedSource.is_active == True).all()
    
    # Store data in memory so we can close the main session before threading
    feed_data = [(s.id, s.name, s.url) for s in sources]
    main_session.close()
    
    if not feed_data:
        log("‚ö†Ô∏è No active feeds found.", source)
        return

    scorer = get_scorer()
    total_added_all = 0
    
    # --- The Multithreading Engine ---
    # Max workers = 15. This means 15 feeds are processed at the exact same time.
    # Do not set this higher than 20 or Postgres might run out of connections.
    with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
        # Submit all tasks to the thread pool
        futures = {
            executor.submit(process_single_feed, f_id, f_name, f_url, scorer): f_name 
            for f_id, f_name, f_url in feed_data
        }
        
        # Collect results as they finish
        for future in concurrent.futures.as_completed(futures):
            feed_name = futures[future]
            try:
                added = future.result()
                total_added_all += added
            except Exception as exc:
                log(f"üí• Thread crashed for {feed_name}: {exc}", source)

    log(f"üèÅ Cycle complete. Added {total_added_all} total new items.", source)

if __name__ == "__main__":
    # Standard 15-minute schedule
    schedule.every(15).minutes.do(fetch_feeds)
    
    # Run once on worker startup
    fetch_feeds(source="Worker Boot")
    
    log("üöÄ Scheduler Service Started...", "SYSTEM")
    while True:
        schedule.run_pending()
        time.sleep(1)